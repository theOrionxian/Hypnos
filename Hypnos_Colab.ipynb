{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "nugrbpNDajBk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbYsUSipQh2N"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers accelerate transformers bitsandbytes einops xformers==0.0.25 nvidia-cutlass huggingface_hub torchmetrics torch-fidelity\n",
        "!pip install carvekit --no-deps"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import copy\n",
        "import gc\n",
        "import hashlib\n",
        "import importlib\n",
        "import itertools\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import shutil\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from einops import rearrange\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.models\n",
        "import torchvision.models.segmentation as segmentation\n",
        "import transformers\n",
        "from accelerate import Accelerator\n",
        "from accelerate.logging import get_logger\n",
        "from accelerate.utils import ProjectConfiguration, set_seed\n",
        "from huggingface_hub import create_repo, model_info, upload_folder\n",
        "from packaging import version\n",
        "from PIL import Image\n",
        "from PIL.ImageOps import exif_transpose\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import AutoTokenizer, PretrainedConfig\n",
        "from transformers import CLIPVisionModel, CLIPTextModel, CLIPProcessor\n",
        "from transformers import ViTImageProcessor, ViTModel\n",
        "import bitsandbytes as bnb\n",
        "import xformers\n",
        "\n",
        "\n",
        "import diffusers\n",
        "from diffusers import (\n",
        "    AutoencoderKL,\n",
        "    DDPMScheduler,\n",
        "    DiffusionPipeline,\n",
        "    StableDiffusionPipeline,\n",
        "    UNet2DConditionModel,\n",
        ")\n",
        "from diffusers.optimization import get_scheduler\n",
        "#from diffusers.utils.training_utils import compute_snr\n",
        "from diffusers.utils import check_min_version, is_wandb_available\n",
        "from diffusers.utils.import_utils import is_xformers_available\n",
        "from diffusers.models.embeddings import get_timestep_embedding\n",
        "\n",
        "from carvekit.api.high import HiInterface\n",
        "\n",
        "from torchmetrics.image.fid import FrechetInceptionDistance\n",
        "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n",
        "from torchmetrics.image import PeakSignalNoiseRatio\n",
        "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VLa0h8jYQpO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Credits and Acknowledgements"
      ],
      "metadata": {
        "id": "K4_r599LkKyM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main structure of this training loop is inspired by the official huggingface implementation of dreambooth https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/train_dreambooth.py"
      ],
      "metadata": {
        "id": "lCXPLZgSkPPt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Params (Please fill these params)"
      ],
      "metadata": {
        "id": "qgCnLqSsan7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATH = \"runwayml/stable-diffusion-v1-5\"  # @param {\"type\":\"string\"}\n",
        "INSTANCE_DIR = \"/content/drive/MyDrive/Hypnos/RatanChair1\" # @param {\"type\":\"string\"}\n",
        "PRESERVE_DIR = \"/content/drive/MyDrive/Hypnos/preserve_images_chair\" # @param {\"type\":\"string\"}\n",
        "LATENT_DISC_DIR = \"/content/drive/Hypnos/hypnos_ld.pt\" # @param {\"type\":\"string\"}\n",
        "OUTPUT_DIR = \"/content/Hypnos-Output\" # @param {\"type\":\"string\"}\n",
        "LOGGING_DIR = \"/content/Hypnos-logging\" # @param {\"type\":\"string\"}\n",
        "GRADIENT_ACCUMULATION_STEPS = 0 # @param {\"type\":\"integer\"}\n",
        "NUM_PRESERVE_IMAGES = 200 # @param {\"type\":\"integer\"}\n",
        "CLASS_NAME = \"chair\" # @param {\"type\":\"string\"}\n",
        "\n",
        "LR = 2e-6 # @param {\"type\":\"number\"}\n",
        "BATCH_SIZE = 1 # @param {\"type\":\"integer\"}\n",
        "WARMUP_STEPS = 0 # @param {\"type\":\"integer\"}\n",
        "TRAIN_STEPS = 800 # @param {\"type\":\"integer\"}\n",
        "GRADIENT_ACCUM = 1 # @param {\"type\":\"integer\"}\n",
        "\n",
        "CHANGE_BG_RATIO = 0.66 # @param {\"type\":\"slider\",\"min\":0,\"max\":1,\"step\":0.01}\n",
        "RESIZED_RATIO = 0.15 # @param {\"type\":\"slider\",\"min\":0,\"max\":1,\"step\":0.01}\n",
        "RESIZED_RATIO *= CHANGE_BG_RATIO\n",
        "STD_DEV = 0.8 # @param {\"type\":\"slider\",\"min\":0,\"max\":5,\"step\":0.05}\n",
        "\n",
        "generate_preservation = True # @param {\"type\":\"boolean\",\"placeholder\":\"Generate Preservation Images\"}\n",
        "train_latent_disc = True # @param {\"type\":\"boolean\",\"placeholder\":\"Train Latent Discriminator\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "RzR-UFrPhoP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Other params\n",
        "torch_dtype = torch.float32\n",
        "REVISION = None\n",
        "INSTANCE_PROMPT = f\"a photo of a sks {CLASS_NAME}\"\n",
        "PRESERVE_PROMPT = f\"a photo of a {CLASS_NAME}\"\n",
        "\n",
        "LAYER_WEIGHT = {\n",
        "    '2': 0.35,\n",
        "    '3': 0.4,\n",
        "    '4': 0.25\n",
        "} #perceptual loss composition"
      ],
      "metadata": {
        "id": "sa8MP_6iiiEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Utils"
      ],
      "metadata": {
        "id": "0GbPQkt3a0-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PromptDataset(Dataset):\n",
        "  def __init__(self, prompt, num_samples):\n",
        "    self.prompt = prompt\n",
        "    self.num_samples = num_samples\n",
        "\n",
        "  def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      example = {}\n",
        "      example[\"prompt\"] = self.prompt\n",
        "      example[\"index\"] = index\n",
        "      return example"
      ],
      "metadata": {
        "id": "8yoZWQ9bVxJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging_dir = Path(OUTPUT_DIR, LOGGING_DIR)\n",
        "accelerator_project_config = ProjectConfiguration(project_dir=OUTPUT_DIR, logging_dir=logging_dir)\n",
        "\n",
        "accelerator = Accelerator(\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    project_config=accelerator_project_config,\n",
        "    mixed_precision=\"fp16\"\n",
        ")"
      ],
      "metadata": {
        "id": "tgMFWIlwWi-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(examples):\n",
        "    has_attention_mask = \"instance_attention_mask\" in examples[0]\n",
        "\n",
        "    no_bg = [example[\"no_background\"] for example in examples]\n",
        "\n",
        "    input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n",
        "    pixel_values = [example[\"instance_image\"] for example in examples]\n",
        "\n",
        "    if has_attention_mask:\n",
        "        attention_mask = [example[\"instance_attention_mask\"] for example in examples]\n",
        "\n",
        "    # Concat class and instance examples for prior preservation.\n",
        "    # We do this to avoid doing two forward passes.\n",
        "    input_ids += [example[\"preserve_prompt_ids\"] for example in examples]\n",
        "    pixel_values += [example[\"preserve_image\"] for example in examples]\n",
        "\n",
        "    if has_attention_mask:\n",
        "        attention_mask += [example[\"preserve_attention_mask\"] for example in examples]\n",
        "\n",
        "    pixel_values = torch.stack(pixel_values)\n",
        "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "\n",
        "    batch = {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"pixel_values\": pixel_values,\n",
        "        \"no_bg\": no_bg\n",
        "    }\n",
        "\n",
        "    if has_attention_mask:\n",
        "        attention_mask = torch.cat(attention_mask, dim=0)\n",
        "        batch[\"attention_mask\"] = attention_mask\n",
        "\n",
        "    return batch"
      ],
      "metadata": {
        "id": "VcHxRe4zWmr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_prompt(prompt, tokenizer, max_length=None):\n",
        "  max_length = max_length if max_length is not None else tokenizer.model_max_length\n",
        "  return tokenizer(\n",
        "      prompt,\n",
        "      truncation=True,\n",
        "      padding=\"max_length\",\n",
        "      max_length=max_length,\n",
        "      return_tensors=\"pt\"\n",
        "  )\n",
        "\n",
        "def encode_prompt(text_encoder, input_ids, attention_mask):\n",
        "  input_ids.to(text_encoder.device)\n",
        "  attention_mask.to(text_encoder.device)\n",
        "  return text_encoder(input_ids, attention_mask)[0]\n",
        "\n",
        "def get_z0(z, pred_noise, t, scheduler, device):\n",
        "  alpha_cumprod = torch.Tensor([scheduler.alphas_cumprod[a] for a in t]).view(-1, 1, 1, 1).to(device)\n",
        "  numerator = z - (1-alpha_cumprod).sqrt() * pred_noise\n",
        "  return numerator / alpha_cumprod.sqrt()\n",
        "\n",
        "\n",
        "\n",
        "std_dev = STD_DEV\n",
        "sqrt_2_pi = (torch.tensor(math.pi) * 2 * std_dev).sqrt().to(device)"
      ],
      "metadata": {
        "id": "f4zKkLVsXmO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_features(image, model, layers):\n",
        "    features = {}\n",
        "    x = image\n",
        "    for name, layer in model._modules.items():\n",
        "        x = layer(x)\n",
        "        if name in layers:\n",
        "            features[name] = x\n",
        "        if name == layers[-1]: break\n",
        "    return features\n",
        "\n",
        "def tracerb7_wrapper(interface):\n",
        "  def inner(images):\n",
        "    res_tensor = None\n",
        "    for im in images:\n",
        "      with torch.no_grad():\n",
        "        im = transforms.ToPILImage(mode=\"RGB\")(im)\n",
        "      res = torch.Tensor(interface([im])[0].getdata()).reshape(*im.size, 4)\n",
        "      res = rearrange(res, \"h w c -> c h w\").unsqueeze(0)\n",
        "      if res_tensor == None:\n",
        "        res_tensor = res\n",
        "      else:\n",
        "        res_tensor = torch.cat((res_tensor, res), 0)\n",
        "    return res_tensor\n",
        "  return inner\n",
        "\n",
        "\n",
        "def get_object(model):\n",
        "  preprocess = transforms.Compose([\n",
        "    transforms.Resize(512, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "  ])\n",
        "  resize = transforms.Compose([\n",
        "    transforms.Resize(512, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "  ])\n",
        "  half_resize = transforms.Compose([\n",
        "    transforms.Resize(256, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "  ])\n",
        "  colors = {\n",
        "      \"black\":  torch.tensor([0, 0, 0]).to(device),\n",
        "      \"white\":  torch.tensor([1, 1, 1]).to(device),\n",
        "      \"red\":  torch.tensor([1, 0, 0]).to(device),\n",
        "      \"green\":  torch.tensor([0, 1, 0]).to(device),\n",
        "      \"blue\":  torch.tensor([0, 0, 1]).to(device),\n",
        "      \"cyan\":  torch.tensor([0, 1, 1]).to(device),\n",
        "      \"magenta\":  torch.tensor([1, 0, 1]).to(device),\n",
        "      \"yellow\":  torch.tensor([1, 1, 0]).to(device),\n",
        "      \"purple\":  torch.tensor([.5, 0, 1]).to(device),\n",
        "      \"pink\":  torch.tensor([1, 0, .5]).to(device),\n",
        "      \"brown\":  torch.tensor([.25, .1, .05]).to(device),\n",
        "      \"gray\":  torch.tensor([.5, .5, .5]).to(device),\n",
        "  }\n",
        "  def inner(images, color=\"black\", down=False, negative=False, maximize=False):\n",
        "    if type(color) == str:\n",
        "      color = colors[color]\n",
        "    in_images = preprocess(0.5*images + 0.5).to(device)\n",
        "    output = model(in_images)[:, 3:, :, :]\n",
        "    mask = torch.where(output == 0, torch.tensor(0.0), torch.tensor(1.0)).to(device)\n",
        "\n",
        "    if down:\n",
        "      empty = (torch.zeros(images.shape) - 0.5).to(device)\n",
        "      empty[:, :, 127:383, 127:383] = half_resize(images)\n",
        "      images = empty\n",
        "\n",
        "      empty = torch.zeros(mask.shape).to(device)\n",
        "      empty[:, :, 127:383, 127:383] = half_resize(mask)\n",
        "      mask = empty\n",
        "\n",
        "    colored_bg = torch.where(rearrange(mask, \"b c h w -> b h w c\") == 0, color, colors[\"black\"])\n",
        "    colored_bg = rearrange(colored_bg, \"b h w c -> b c h w\").to(device)\n",
        "    unorm_image = (0.5*resize(images) + 0.5).to(device)\n",
        "    if negative:\n",
        "      unorm_image = 1 - unorm_image\n",
        "    return transforms.Normalize([0.5], [0.5])((unorm_image * mask) + colored_bg)\n",
        "  return inner\n",
        "\n",
        "def foreground_perceptual_loss(classifier, segmentator, layer_weight):\n",
        "  get_ob = get_object(segmentator)\n",
        "  norm = transforms.Compose([\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "  ])\n",
        "  def preprocess(image):\n",
        "    return norm(image/2 + 0.5)\n",
        "  def inner(real_images, fake_images):\n",
        "    \"\"\"Calculate the perceptual loss between real and fake images.\"\"\"\n",
        "    layers = list(layer_weight.keys())\n",
        "\n",
        "    back_color = (torch.rand(size=(1,)) / 1.25).tile(3).to(device)\n",
        "\n",
        "    #real_images, fake_images = get_ob(real_images, color=back_color), get_ob(fake_images, color=back_color)\n",
        "    real_features = get_features(preprocess(real_images), classifier, layers)\n",
        "    fake_features = get_features(preprocess(fake_images), classifier, layers)\n",
        "\n",
        "    loss = 0.0\n",
        "    for name in layers:\n",
        "      loss += layer_weight[name] * torch.mean((real_features[name] - fake_features[name]) ** 2)\n",
        "\n",
        "    return loss\n",
        "  return inner, get_ob"
      ],
      "metadata": {
        "id": "85p-F6nKcp4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make dataset\n",
        "class DreamDataset(Dataset):\n",
        "  def __init__(self, instance_path, instance_prompt, preserve_path, preserve_prompt, tokenizer, size=512, change_bg_ratio=0.75, resized_ratio=8/15):\n",
        "    self.preserve_path = Path(preserve_path)\n",
        "    self.preserve_path.mkdir(parents=True, exist_ok=True)\n",
        "    self.preserve_images_path = list(self.preserve_path.iterdir())\n",
        "    self.num_preserve_images = len(self.preserve_images_path)\n",
        "    self.preserve_prompt = preserve_prompt\n",
        "\n",
        "    self.size = size\n",
        "    self.tokenizer = tokenizer\n",
        "    self.instance_path = Path(instance_path)\n",
        "    self.images_path = list(self.instance_path.iterdir())\n",
        "    self.instance_prompt = instance_prompt\n",
        "    self.instance_images_path = list(self.instance_path.iterdir())\n",
        "    self.num_instance_images = len(self.instance_images_path)\n",
        "    self.image_transforms = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "                transforms.CenterCrop(size),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.5], [0.5]),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    self.colors = [\n",
        "        \"black\",\n",
        "        \"white\",\n",
        "        \"red\",\n",
        "        \"green\",\n",
        "        \"blue\",\n",
        "        \"cyan\",\n",
        "        \"magenta\",\n",
        "        \"yellow\",\n",
        "        \"purple\",\n",
        "        \"pink\",\n",
        "        \"brown\"\n",
        "    ]\n",
        "    self.normal_ratio = 1.0 - change_bg_ratio\n",
        "    self.resize_thresh = 1.0 - (change_bg_ratio * resized_ratio)\n",
        "\n",
        "  def __len__(self):\n",
        "    return max(self.num_instance_images, self.num_preserve_images)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    items = {}\n",
        "    instance_image = Image.open(self.instance_images_path[index % self.num_instance_images])\n",
        "    instance_image = exif_transpose(instance_image)\n",
        "\n",
        "    if not instance_image.mode ==\"RGB\":\n",
        "      instance_image = instance_image.convert(\"RGB\")\n",
        "    items[\"instance_image\"] = self.image_transforms(instance_image)\n",
        "\n",
        "    self.no_background = torch.rand((1,))[0] > self.normal_ratio\n",
        "    items[\"no_background\"] = self.no_background\n",
        "    if self.no_background:\n",
        "      color = random.choice(self.colors)\n",
        "      self.instance_prompt += f\", {color} background\"\n",
        "      items[\"instance_image\"] = get_ob(items[\"instance_image\"].unsqueeze(0), color, torch.rand((1,))[0] > self.resize_thresh)[0].cpu()\n",
        "    else:\n",
        "      self.instance_prompt += \", vfx background\"\n",
        "\n",
        "    instance_token = tokenize_prompt(self.instance_prompt, self.tokenizer)\n",
        "    items['instance_prompt_ids'] = instance_token.input_ids\n",
        "    items['instance_attention_mask'] = instance_token.attention_mask\n",
        "\n",
        "    if self.preserve_path:\n",
        "      preserve_image = Image.open(self.preserve_images_path[index % self.num_preserve_images])\n",
        "      preserve_image = exif_transpose(preserve_image)\n",
        "      if not preserve_image.mode ==\"RGB\":\n",
        "        preserve_image = preserve_image.convert(\"RGB\")\n",
        "      items[\"preserve_image\"] = self.image_transforms(preserve_image)\n",
        "\n",
        "      preserve_token = tokenize_prompt(self.preserve_prompt, self.tokenizer)\n",
        "      items['preserve_prompt_ids'] = preserve_token.input_ids\n",
        "      items['preserve_attention_mask'] = preserve_token.attention_mask\n",
        "\n",
        "    return items\n"
      ],
      "metadata": {
        "id": "EQm_br1MciGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LDreamDataset(Dataset):\n",
        "  def __init__(self,instance_path, preserve_path, size=512):\n",
        "    self.instance_path = Path(instance_path)\n",
        "    self.instance_images_path = list(self.instance_path.iterdir())\n",
        "    self.num_instance_images = len(self.instance_images_path)\n",
        "\n",
        "    self.preserve_path = Path(preserve_path)\n",
        "    self.preserve_images_path = list(self.preserve_path.iterdir())\n",
        "    self.num_preserve_images = len(self.preserve_images_path)\n",
        "    self.image_transforms = transforms.Compose(\n",
        "          [\n",
        "              transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "              transforms.CenterCrop(size),\n",
        "              transforms.ToTensor(),\n",
        "              transforms.Normalize([0.5], [0.5]),\n",
        "          ]\n",
        "      )\n",
        "    self.colors = [\n",
        "        \"black\",\n",
        "        \"white\",\n",
        "        \"red\",\n",
        "        \"green\",\n",
        "        \"blue\",\n",
        "        \"cyan\",\n",
        "        \"magenta\",\n",
        "        \"yellow\",\n",
        "        \"purple\",\n",
        "        \"pink\",\n",
        "        \"brown\"\n",
        "    ]\n",
        "\n",
        "  def __len__(self):\n",
        "    return max(self.num_instance_images, self.num_preserve_images)*2\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    mode = random.uniform(0, 1)\n",
        "    if index%2:\n",
        "      instance_image = Image.open(self.instance_images_path[index % self.num_instance_images])\n",
        "      instance_image = exif_transpose(instance_image)\n",
        "\n",
        "      if not instance_image.mode ==\"RGB\":\n",
        "        instance_image = instance_image.convert(\"RGB\")\n",
        "      instance_image = self.image_transforms(instance_image)\n",
        "      if mode > 0.6:\n",
        "        instance_image =  get_ob(instance_image.unsqueeze(0), random.choice(self.colors), mode > 0.9)[0].cpu()\n",
        "      return instance_image, torch.Tensor([1])\n",
        "    else:\n",
        "      if mode < 0.35:\n",
        "        preserve_image = Image.open(self.preserve_images_path[index % self.num_preserve_images])\n",
        "        preserve_image = exif_transpose(preserve_image)\n",
        "        if not preserve_image.mode ==\"RGB\":\n",
        "          preserve_image = preserve_image.convert(\"RGB\")\n",
        "        preserve_image = self.image_transforms(preserve_image)\n",
        "      else:\n",
        "        is_negative = random.uniform(0, 1) < 0.25\n",
        "        instance_image = Image.open(self.instance_images_path[index % self.num_instance_images])\n",
        "        instance_image = exif_transpose(instance_image)\n",
        "\n",
        "        if not instance_image.mode ==\"RGB\":\n",
        "          instance_image = instance_image.convert(\"RGB\")\n",
        "        instance_image = self.image_transforms(instance_image)\n",
        "        if mode > 0.6:\n",
        "          instance_image =  get_ob(instance_image.unsqueeze(0), random.choice(self.colors), False, is_negative)[0].cpu()\n",
        "          if is_negative:\n",
        "            preserve_image = instance_image\n",
        "          else:\n",
        "            preserve_image = transforms.Normalize([0.5], [0.5])((0.5*instance_image +0.5) - (0.5*get_ob(instance_image.unsqueeze(0), \"black\", False)[0].cpu() + 0.5))\n",
        "        else:\n",
        "          if is_negative:\n",
        "            background = (0.5*instance_image +0.5) - (0.5*get_ob(instance_image.unsqueeze(0), \"black\", False)[0].cpu() + 0.5)\n",
        "            instance_image = (0.5*get_ob(instance_image.unsqueeze(0), \"black\", False, True)[0].cpu() + 0.5)\n",
        "            preserve_image = transforms.Normalize([0.5], [0.5])(instance_image + background)\n",
        "          else:\n",
        "            preserve_image = transforms.Normalize([0.5], [0.5])((0.5*instance_image +0.5) - (0.5*get_ob(instance_image.unsqueeze(0), \"black\", False)[0].cpu() + 0.5))\n",
        "      return preserve_image, torch.Tensor([0])\n",
        "\n"
      ],
      "metadata": {
        "id": "A_c9e0GqcxDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, in_channel, out_channel):\n",
        "    super().__init__()\n",
        "    self.norm = nn.LayerNorm(in_channel)\n",
        "    self.mha = nn.MultiheadAttention(in_channel, 4, batch_first=True)\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.LayerNorm(in_channel),\n",
        "        nn.Linear(in_channel, out_channel),\n",
        "        nn.SiLU()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    xn = self.norm(x)\n",
        "    xm, aw = self.mha(xn, xn, xn, average_attn_weights=False)\n",
        "    x = x + xm\n",
        "    return self.mlp(x), aw\n",
        "\n",
        "class ExtractPatch(nn.Module):\n",
        "  def __init__(self, dim, patch_dim, in_channel):\n",
        "    super().__init__()\n",
        "    self.dim = dim\n",
        "    self.patch_dim = patch_dim\n",
        "    self.lin = nn.Linear(patch_dim*patch_dim*in_channel, patch_dim*patch_dim*in_channel)\n",
        "    self.cls = nn.Parameter(torch.randn(1, 1, 128))\n",
        "    self.out = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.out = None\n",
        "    position = torch.Tensor([0.])\n",
        "    for i in range(self.dim//self.patch_dim):\n",
        "      for j in range(self.dim//self.patch_dim):\n",
        "        patch  = rearrange(x[:, :, i*self.patch_dim:i*self.patch_dim+4, j*self.patch_dim:j*self.patch_dim+4], \"b c h w -> b (c h w)\")\n",
        "        patch = self.lin(patch).unsqueeze(1)\n",
        "        pos_encode = torch.tile(get_timestep_embedding(position, 128), (patch.shape[0], 1)).unsqueeze(1).to(device)\n",
        "        patch = patch + pos_encode\n",
        "        if self.out is None:\n",
        "          self.out = patch\n",
        "        else:\n",
        "          self.out = torch.cat([self.out, patch], axis=1)\n",
        "        position = position + 1\n",
        "    cls = torch.tile(self.cls, (patch.shape[0], 1, 1))\n",
        "    self.out = torch.cat([cls, self.out], axis=1)\n",
        "    return self.out\n",
        "\n",
        "\n",
        "\n",
        "class MergePatch(nn.Module):\n",
        "  def __init__(self, dim, patch_dim, in_channel):\n",
        "    super().__init__()\n",
        "    self.dim = dim\n",
        "    self.patch_dim = patch_dim\n",
        "    self.num_patch_per_col = dim//patch_dim\n",
        "    self.in_channel = in_channel\n",
        "    self.column = None\n",
        "    self.out = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.column = None\n",
        "    self.out = None\n",
        "    c, x = x[:, 0, :], x[:, 1:, :]\n",
        "    for i in range(x.shape[1]):\n",
        "      patch = rearrange(x[:, i, :], \"b (c h w) -> b c h w\", c=self.in_channel, h=self.patch_dim, w=self.patch_dim)\n",
        "      if self.column is None:\n",
        "          self.column = patch\n",
        "      else:\n",
        "        self.column = torch.cat([self.column, patch], axis=3)\n",
        "      if (i+1) % self.num_patch_per_col == 0:\n",
        "        if self.out is None:\n",
        "          self.out = self.column\n",
        "        else:\n",
        "          self.out = torch.cat([self.out, self.column], axis=2)\n",
        "        self.column = None\n",
        "    return c, self.out\n",
        "\n",
        "\n",
        "class LatentDiscOld(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.downconv = nn.Conv2d(4, 8, kernel_size=4, stride=2, padding=1)\n",
        "    self.conv1 = nn.Sequential(\n",
        "        nn.Conv2d(8, 8, kernel_size=3, padding=1),\n",
        "        nn.LayerNorm(32),\n",
        "        nn.SiLU()\n",
        "    )\n",
        "    self.extract = ExtractPatch(32, 4, 8)\n",
        "    self.block1 = TransformerBlock(128, 64)\n",
        "    self.block2 = TransformerBlock(64, 32)\n",
        "    self.block3 = TransformerBlock(32, 16)\n",
        "    self.head = nn.Sequential(\n",
        "        nn.Linear(1024, 128),\n",
        "        nn.SiLU(),\n",
        "        nn.Linear(128, 1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.downconv(x)\n",
        "    x = self.conv1(x)\n",
        "    x = self.extract(x)\n",
        "    x, _ = self.block1(x)\n",
        "    x, _ = self.block2(x)\n",
        "    x, _ = self.block3(x)\n",
        "    x = rearrange(x, \"b s e -> b (s e)\")\n",
        "    return self.head(x)\n",
        "\n",
        "class LatentDisc(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Sequential(\n",
        "        nn.Conv2d(4, 8, kernel_size=3, padding=1),\n",
        "        nn.LayerNorm(64),\n",
        "        nn.SiLU(),\n",
        "        nn.Conv2d(8, 4, kernel_size=3, padding=1),\n",
        "        nn.LayerNorm(64),\n",
        "        nn.SiLU()\n",
        "    )\n",
        "    self.extract = ExtractPatch(64, 4, 8)\n",
        "    self.block1 = TransformerBlock(128, 128)\n",
        "    self.block2 = TransformerBlock(128, 128)\n",
        "    self.block3 = TransformerBlock(128, 128)\n",
        "    self.head = nn.Sequential(\n",
        "        nn.Linear(128, 32),\n",
        "        nn.SiLU(),\n",
        "        nn.Linear(32, 16),\n",
        "        nn.SiLU(),\n",
        "        nn.Linear(16, 1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    xc = self.conv1(x)\n",
        "    x = self.extract(torch.cat([x, xc], axis=1))\n",
        "    x,_ = self.block1(x)\n",
        "    x,_ = self.block2(x)\n",
        "    x,_ = self.block3(x)\n",
        "    return self.head(x[:, 0, :])\n",
        "\n",
        "class LatentDiscWithConv(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.downconv = nn.Conv2d(4, 8, kernel_size=4, stride=2, padding=1)\n",
        "    self.conv1 = nn.Sequential(\n",
        "        nn.Conv2d(8, 8, kernel_size=3, padding=1),\n",
        "        nn.LayerNorm(32),\n",
        "        nn.SiLU()\n",
        "    )\n",
        "    self.extract = ExtractPatch(32, 4, 8)\n",
        "    self.block1 = TransformerBlock(128, 128)\n",
        "    self.block2 = TransformerBlock(128, 128)\n",
        "    self.block3 = TransformerBlock(128, 128)\n",
        "    self.merge = MergePatch(32, 4, 8),\n",
        "    self.conv2 = nn.Sequential(\n",
        "\n",
        "    )\n",
        "    self.head = nn.Sequential(\n",
        "        nn.Linear(128, 64),\n",
        "        nn.SiLU(),\n",
        "        nn.Linear(64, 32),\n",
        "        nn.SiLU(),\n",
        "        nn.Linear(32, 1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.downconv(x)\n",
        "    x = self.conv1(x)\n",
        "    x = self.extract(x)\n",
        "    x = self.block1(x)\n",
        "    x = self.block2(x)\n",
        "    x = self.block3(x)\n",
        "    _, x = self.merge(x)\n",
        "    return self.head(x)\n",
        "\n",
        "class LatentBGRemover(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(4, 8, kernel_size=3, padding=1)\n",
        "    self.downconv = nn.Sequential(\n",
        "        nn.Conv2d(8, 8, kernel_size=4, stride=2, padding=1),\n",
        "        nn.LayerNorm(32),\n",
        "        nn.SiLU()\n",
        "    )\n",
        "    self.upconv = nn.Sequential(\n",
        "        nn.LayerNorm(32),\n",
        "        nn.ConvTranspose2d(16, 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    )\n",
        "    self.extract = ExtractPatch(32, 4, 8)\n",
        "    self.block1 = TransformerBlock(128, 64)\n",
        "    self.block2 = TransformerBlock(64, 64)\n",
        "    self.block3 = TransformerBlock(64, 64)\n",
        "    self.block4 = TransformerBlock(64, 128)\n",
        "    self.merge = MergePatch(32, 4, 8)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x0 = self.conv1(x)\n",
        "    x = self.dowwnconv(x0)\n",
        "    x1 = self.extract(x)\n",
        "    x2 = self.block1(x1)\n",
        "    x3 = self.block2(x2)\n",
        "\n",
        "    x = self.block3(x3)\n",
        "    x = self.block4(x + x2)\n",
        "    x = self.merge(x)\n",
        "    return self.upconv(torch.cat([x, x0], axis=1))"
      ],
      "metadata": {
        "id": "303c55TtczDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Make Preservation Image"
      ],
      "metadata": {
        "id": "ZfiEVb35a3wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not generate_preservation:\n",
        "  print(\"Preservation Images Generation turned off\")\n",
        "  return\n",
        "\n",
        "pipeline = DiffusionPipeline.from_pretrained(\n",
        "                MODEL_PATH,\n",
        "                torch_dtype=torch.float16,\n",
        "                safety_checker=None,\n",
        "                revision=REVISION,\n",
        "            )\n",
        "pipeline.set_progress_bar_config(disable=True)\n",
        "\n",
        "preserve_dir = Path(PRESERVE_DIR)\n",
        "if not preserve_dir.exists(): preserve_dir.mkdir(parents=True)\n",
        "existing_num_preserve = len(list(preserve_dir.iterdir()))\n",
        "num_generate = max(NUM_PRESERVE_IMAGES - existing_num_preserve, 0)\n",
        "\n",
        "preserve_prompt_ds = PromptDataset(PRESERVE_PROMPT, num_generate)\n",
        "preserve_prompt_loader = DataLoader(preserve_prompt_ds, batch_size=4)\n",
        "preserve_prompt_loader = accelerator.prepare(preserve_prompt_loader)\n",
        "\n",
        "pipeline.to(accelerator.device)\n",
        "\n",
        "for batch in tqdm(preserve_prompt_loader, desc=\"Generating Images\", disable=not accelerator.is_local_main_process):\n",
        "  images = pipeline(batch[\"prompt\"]).images\n",
        "\n",
        "  for i, image in enumerate(images):\n",
        "    hashed = hashlib.sha1(image.tobytes()).hexdigest()\n",
        "    image_index = batch[\"index\"][i] + existing_num_preserve\n",
        "    image_filename = preserve_dir / f\"{image_index}_{hashed}.jpg\"\n",
        "    image.save(image_filename)\n",
        "\n",
        "del pipeline"
      ],
      "metadata": {
        "id": "Csg_G7poXtEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initialization"
      ],
      "metadata": {
        "id": "t0Q5iRuXbN03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "            MODEL_PATH,\n",
        "            subfolder=\"tokenizer\",\n",
        "            revision=REVISION,\n",
        "            use_fast=False,\n",
        "      )\n",
        "\n",
        "noise_scheduler = DDPMScheduler.from_pretrained(MODEL_PATH, subfolder=\"scheduler\")\n",
        "text_encoder = CLIPTextModel.from_pretrained(MODEL_PATH, subfolder=\"text_encoder\", revision=REVISION)\n",
        "vae = AutoencoderKL.from_pretrained(\n",
        "            MODEL_PATH, subfolder=\"vae\", revision=REVISION\n",
        "      )\n",
        "unet = UNet2DConditionModel.from_pretrained(\n",
        "        MODEL_PATH, subfolder=\"unet\", revision=REVISION\n",
        "      )"
      ],
      "metadata": {
        "id": "Ya56BRjgbIk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_to_optimize = (itertools.chain(unet.parameters(), text_encoder.parameters()))\n",
        "\n",
        "optimizer = bnb.optim.AdamW8bit(\n",
        "    params_to_optimize,\n",
        "    lr = LR,\n",
        "    betas=(.9, .999),\n",
        "    weight_decay=1e-2,\n",
        "    eps=1e-8\n",
        ")\n"
      ],
      "metadata": {
        "id": "2tS1BikiX_hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enetb1 = torchvision.models.efficientnet_b1(pretrained=True).features.to(device)\n",
        "enetb1.eval()\n",
        "\n",
        "# Check doc strings for more information\n",
        "interface = HiInterface(object_type=\"object\",  # Can be \"object\" or \"hairs-like\".\n",
        "                        batch_size_seg=5,\n",
        "                        batch_size_matting=1,\n",
        "                        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
        "                        seg_mask_size=640,  # Use 640 for Tracer B7 and 320 for U2Net\n",
        "                        matting_mask_size=2048,\n",
        "                        trimap_prob_threshold=231,\n",
        "                        trimap_dilation=30,\n",
        "                        trimap_erosion_iters=5,\n",
        "                        fp16=True)"
      ],
      "metadata": {
        "id": "iqzFvBSBbRvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tracerb7 = tracerb7_wrapper(interface)\n",
        "\n",
        "foreground_perceptual, get_ob = foreground_perceptual_loss(\n",
        "    classifier = enetb1,\n",
        "    segmentator = tracerb7,\n",
        "    layer_weight = LAYER_WEIGHT\n",
        ")"
      ],
      "metadata": {
        "id": "78-UUOHfblfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = DreamDataset(\n",
        "    instance_path=INSTANCE_DIR,\n",
        "    instance_prompt=INSTANCE_PROMPT,\n",
        "    preserve_path=PRESERVE_DIR,\n",
        "    preserve_prompt=PRESERVE_PROMPT,\n",
        "    tokenizer=tokenizer,\n",
        "    change_bg_ratio=CHANGE_BG_RATIO,\n",
        "    resized_ratio=0\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda e: collate_fn(e)\n",
        ")\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "        \"constant\",\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=WARMUP_STEPS,\n",
        "        num_training_steps=TRAIN_STEPS,\n",
        "        num_cycles=1,\n",
        "    )\n",
        "\n",
        "weight_dtype = torch.float32\n",
        "if accelerator.mixed_precision == \"fp16\":\n",
        "    weight_dtype = torch.float16\n",
        "elif accelerator.mixed_precision == \"bf16\":\n",
        "    weight_dtype = torch.bfloat16\n",
        "\n",
        "unet, text_encoder, optimizer, train_loader, lr_scheduler = accelerator.prepare(\n",
        "            unet, text_encoder, optimizer, train_loader, lr_scheduler\n",
        "        )\n",
        "\n",
        "vae.to(accelerator.device, dtype=torch.float16)\n",
        "text_encoder.to(accelerator.device, dtype=torch.float32)\n",
        "\n",
        "num_updating = math.ceil(len(train_loader) / GRADIENT_ACCUM)\n",
        "num_epoch = math.ceil(TRAIN_STEPS / num_updating)\n",
        "step_counter = 0"
      ],
      "metadata": {
        "id": "ygCQtMwcbsDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Latent Discriminator Training"
      ],
      "metadata": {
        "id": "nXHgUFOicVvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not train_latent_disc:\n",
        "  ld_mod = LatentDisc().to(device)\n",
        "  ld_mod.load_state_dict(torch.load(LATENT_DISC_DIR))\n",
        "  return\n",
        "\n",
        "latent_ds = LDreamDataset(\n",
        "    instance_path=INSTANCE_DIR,\n",
        "    preserve_path=PRESERVE_DIR,\n",
        ")\n",
        "\n",
        "latent_loader = DataLoader(\n",
        "    latent_ds,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "ld_mod = LatentDisc().to(device)\n",
        "\n",
        "optimizer_ld = torch.optim.AdamW(\n",
        "    ld_mod.parameters(),\n",
        "    lr = 1e-4,\n",
        "    betas=(.9, .999),\n",
        "    weight_decay=1e-2,\n",
        "    eps=1e-8\n",
        ")\n",
        "\n",
        "progress_bar_ld = tqdm(\n",
        "    range(len(latent_ds)//4*6),\n",
        "    initial=0,\n",
        "    desc=\"Steps\",\n",
        ")\n",
        "losses = []\n",
        "\n",
        "for epoch in range(6):\n",
        "  if epoch >= 4:\n",
        "    for g in optimizer_ld.param_groups:\n",
        "      g['lr'] = 1e-5\n",
        "  for step, (image, label) in enumerate(latent_loader):\n",
        "    image = image.to(dtype=torch.float16).to(device)\n",
        "    latent = vae.encode(image).latent_dist.sample()\n",
        "    latent *= vae.config.scaling_factor\n",
        "\n",
        "    pred = ld_mod(latent.to(dtype=torch.float32))\n",
        "    loss = nn.MSELoss()(label.to(device), pred)\n",
        "    losses.append(float(loss.mean()))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer_ld.step()\n",
        "    optimizer_ld.zero_grad()\n",
        "    progress_bar_ld.set_description(f\"loss : {float(loss.mean())}\")\n",
        "    progress_bar_ld.update(1)\n",
        "\n",
        "torch.save(ld_mod.state_dict(), LATENT_DISC_DIR)"
      ],
      "metadata": {
        "id": "sE8uytSlb4SZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "0WyEb1VXdXoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ld_mod.requires_grad_(True)\n",
        "optimizer_ld = torch.optim.AdamW(\n",
        "    ld_mod.parameters(),\n",
        "    lr = 1e-5,\n",
        "    betas=(.9, .999),\n",
        "    weight_decay=1e-2,\n",
        "    eps=1e-8\n",
        ")"
      ],
      "metadata": {
        "id": "enMODivjc5AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "progress_bar = tqdm(\n",
        "    range(TRAIN_STEPS),\n",
        "    initial=0,\n",
        "    desc=\"Steps\",\n",
        "    disable=not accelerator.is_local_main_process\n",
        ")\n",
        "\n",
        "losses = []\n",
        "\n",
        "for epoch in range(1, num_epoch+1):\n",
        "  unet.train()\n",
        "  text_encoder.train()\n",
        "\n",
        "  for step, batch in enumerate(train_loader):\n",
        "    with torch.no_grad():\n",
        "      latent = vae.encode(batch[\"pixel_values\"].to(dtype=torch.float16)).latent_dist.sample()\n",
        "      latent *= vae.config.scaling_factor\n",
        "\n",
        "      # sample noise and timestep\n",
        "      noise = torch.randn_like(latent)\n",
        "\n",
        "      b_size, channel, height, width = latent.shape\n",
        "      t = torch.randint(\n",
        "          0,\n",
        "          noise_scheduler.config.num_train_timesteps,\n",
        "          (b_size,),\n",
        "          device=latent.device\n",
        "      ).long()\n",
        "\n",
        "      noisy_latent = noise_scheduler.add_noise(latent, noise, t).to(dtype=torch.float32)\n",
        "\n",
        "    with accelerator.accumulate(unet):\n",
        "\n",
        "      text_embed = encode_prompt(\n",
        "          text_encoder,\n",
        "          batch[\"input_ids\"],\n",
        "          batch[\"attention_mask\"],\n",
        "      ).to(dtype=torch.float32)\n",
        "\n",
        "      if accelerator.unwrap_model(unet).config.in_channels == channel * 2:\n",
        "        noisy_latent = torch.cat([noisy_latent, noisy_latent], dim=1).to(dtype=torch.float32)\n",
        "\n",
        "      #predict\n",
        "      pred_unet = unet(\n",
        "          noisy_latent, t, text_embed, class_labels=None\n",
        "      ).sample\n",
        "\n",
        "      if pred_unet.shape[1] == 6:\n",
        "        pred_unet, _ = torch.chunk(pred_unet, 2, dim=1)\n",
        "\n",
        "      pred_unet_i, pred_unet_p = torch.chunk(pred_unet, 2, dim=0)\n",
        "      noisy_latent_i, noisy_latent_p = torch.chunk(noisy_latent, 2, dim=0)\n",
        "      noise_i, noise_p = torch.chunk(noise, 2, dim=0)\n",
        "      t_i, t_p = torch.chunk(t, 2, dim=0)\n",
        "\n",
        "      z0_i = get_z0(noisy_latent_i.to(device), pred_unet_i.to(device), t_i.to(device), noise_scheduler, device)\n",
        "\n",
        "      #Loss\n",
        "      preserve_loss = F.mse_loss(\n",
        "          pred_unet_p.float(), noise_p.float(),\n",
        "          reduction=\"mean\"\n",
        "      )\n",
        "\n",
        "      instance_loss = sqrt_2_pi * ((F.mse_loss(\n",
        "          pred_unet_i.float(), noise_i.float(),\n",
        "          reduction=\"mean\"\n",
        "      )*(0.5 / std_dev**2)).exp() - 1)\n",
        "\n",
        "      fp_loss = 0\n",
        "      if step_counter <= 500:\n",
        "        fp_loss = foreground_perceptual(\n",
        "            batch[\"pixel_values\"][0].unsqueeze(0)\n",
        "            , vae.decode(z0_i.to(dtype=torch.float16) / vae.config.scaling_factor).sample.to(dtype=torch.float32)\n",
        "        )\n",
        "\n",
        "      lp_loss = 0\n",
        "      lp_loss = F.mse_loss(\n",
        "        torch.Tensor([[1.]]).tile(1, b_size//2).to(device).float(), ld_mod(z0_i.float()),\n",
        "      )\n",
        "\n",
        "      loss = instance_loss + preserve_loss + 3e-3*fp_loss + .5*lp_loss\n",
        "      losses.append((t, instance_loss, preserve_loss, 3e-3*fp_loss, .5*lp_loss))\n",
        "\n",
        "      #backprop\n",
        "      accelerator.backward(loss)\n",
        "      if accelerator.sync_gradients:\n",
        "        params_to_clip = (\n",
        "            itertools.chain(unet.parameters(), text_encoder.parameters())\n",
        "        )\n",
        "        accelerator.clip_grad_norm_(params_to_clip, 1.0)\n",
        "\n",
        "      optimizer.step()\n",
        "      lr_scheduler.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "    #Discriminator\n",
        "    with torch.no_grad():\n",
        "      text_embed = encode_prompt(\n",
        "          text_encoder,\n",
        "          batch[\"input_ids\"],\n",
        "          batch[\"attention_mask\"],\n",
        "      ).to(dtype=torch.float32)\n",
        "\n",
        "      pred_unet = unet(\n",
        "          noisy_latent, t, text_embed, class_labels=None\n",
        "      ).sample\n",
        "\n",
        "      pred_unet_i, pred_unet_p = torch.chunk(pred_unet, 2, dim=0)\n",
        "      noisy_latent_i, noisy_latent_p = torch.chunk(noisy_latent, 2, dim=0)\n",
        "      t_i, t_p = torch.chunk(t, 2, dim=0)\n",
        "\n",
        "      z0_i = get_z0(noisy_latent_i.to(device), pred_unet_i.to(device), t_i.to(device), noise_scheduler, device)\n",
        "\n",
        "    latent_ld = torch.cat([latent[:1, :, :, :], z0_i], dim=0)\n",
        "    pred_ld = ld_mod(latent_ld.to(dtype=torch.float32))\n",
        "    loss_ld = nn.MSELoss()(torch.Tensor([[1.], [0.]]).float().to(device), pred_ld)\n",
        "\n",
        "    loss_ld.backward()\n",
        "    optimizer_ld.step()\n",
        "    optimizer_ld.zero_grad()\n",
        "\n",
        "    if accelerator.sync_gradients:\n",
        "      step_counter += 1\n",
        "      if step_counter % GRADIENT_ACCUM == 0:\n",
        "        progress_bar.set_description(f\"loss : {float(loss)}, ld_loss : {float(loss_ld)}\")\n",
        "        progress_bar.update(1)"
      ],
      "metadata": {
        "id": "VEJ86SDZc-Yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model saving\n",
        "accelerator.wait_for_everyone()\n",
        "if accelerator.is_main_process:\n",
        "  pipeline = DiffusionPipeline.from_pretrained(\n",
        "      MODEL_PATH,\n",
        "      unet=accelerator.unwrap_model(unet),\n",
        "      text_encoder=accelerator.unwrap_model(text_encoder),\n",
        "      revision=REVISION\n",
        "  )\n",
        "\n",
        "  scheduler_args = {}\n",
        "\n",
        "  if \"variance_type\" in pipeline.scheduler.config:\n",
        "      variance_type = pipeline.scheduler.config.variance_type\n",
        "      if variance_type in [\"learned\", \"learned_range\"]:\n",
        "          variance_type = \"fixed_small\"\n",
        "      scheduler_args[\"variance_type\"] = variance_type\n",
        "  pipeline.scheduler = pipeline.scheduler.from_config(pipeline.scheduler.config, **scheduler_args)\n",
        "\n",
        "  pipeline.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "accelerator.end_training()"
      ],
      "metadata": {
        "id": "MUwXVNjSmS3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Trained Model"
      ],
      "metadata": {
        "id": "qhU0AXXQn6DF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = StableDiffusionPipeline.from_pretrained(OUTPUT_DIR, torch_dtype=torch.float16, use_safetensors=True, safety_checker=None).to(device)"
      ],
      "metadata": {
        "id": "U1N_Z81Mn8P8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation Initialization"
      ],
      "metadata": {
        "id": "mIDs7qFVke-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseMetric():\n",
        "  def __init__(self):\n",
        "    self.count = torch.Tensor([0.]).to(device)\n",
        "    self.score = torch.Tensor([0.]).to(device)\n",
        "    self.std = torch.Tensor([0.]).to(device)\n",
        "    self.score_list = torch.Tensor([]).to(device)\n",
        "\n",
        "  def compute(self):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def update(self, instance_path, image):\n",
        "    s = self.compute(instance_path, image)\n",
        "    self.score = self.score * (self.count/(self.count+1.)) + s * (1./(self.count+1.))\n",
        "    self.count = self.count + 1\n",
        "    self.score_list = torch.cat([self.score_list, s.ravel()], axis=0)\n",
        "    self.std = ((self.score_list - self.score) ** 2).mean()\n",
        "    return self\n",
        "\n",
        "  def reset(self):\n",
        "    self.count = torch.Tensor([0.]).to(device)\n",
        "    self.score = torch.Tensor([0.]).to(device)\n",
        "    self.std = torch.Tensor([0.]).to(device)\n",
        "    self.score_list = torch.Tensor([]).to(device)\n",
        "    return self\n",
        "\n",
        "  def print(self):\n",
        "    print(f\"{self.__class__.__name__} : {self.score} ± {self.std}\")\n",
        "\n",
        "class DINO_metric(BaseMetric):\n",
        "  def __init__(self, processor, model):\n",
        "    super().__init__()\n",
        "    self.processor = processor\n",
        "    self.model = model\n",
        "\n",
        "  def compute(self, instance_path, image):\n",
        "    with torch.no_grad():\n",
        "      inputs = self.processor(images=image, return_tensors=\"pt\").to(device)\n",
        "      outputs = self.model(**inputs)\n",
        "      image_emb = outputs.last_hidden_state[0]\n",
        "\n",
        "      buffer = None\n",
        "      for ins_path in list(Path(instance_path).iterdir()):\n",
        "        instance = exif_transpose(Image.open(ins_path))\n",
        "        inputs = self.processor(images=instance, return_tensors=\"pt\").to(device)\n",
        "        outputs = self.model(**inputs)\n",
        "        ins_emb = outputs.last_hidden_state[0]\n",
        "        cos = torch.abs(F.cosine_similarity(image_emb, ins_emb).unsqueeze(0)[:, 0])\n",
        "        if buffer == None:\n",
        "          buffer = cos\n",
        "        else:\n",
        "          buffer = torch.cat([buffer, cos], axis=0)\n",
        "    return torch.mean(buffer)\n",
        "\n",
        "\n",
        "class CLIP_I_metric(BaseMetric):\n",
        "  def __init__(self, processor, model):\n",
        "    super().__init__()\n",
        "    self.processor = processor\n",
        "    self.model = model\n",
        "\n",
        "  def compute(self, instance_path, image):\n",
        "    with torch.no_grad():\n",
        "      inputs = self.processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
        "      outputs = self.model(inputs)\n",
        "      image_emb = outputs[1]\n",
        "\n",
        "      buffer = None\n",
        "      for ins_path in list(Path(instance_path).iterdir()):\n",
        "        instance = exif_transpose(Image.open(ins_path))\n",
        "        inputs = self.processor(images=instance, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
        "        outputs = self.model(inputs)\n",
        "        ins_emb = outputs[1]\n",
        "        cos = torch.abs(F.cosine_similarity(image_emb, ins_emb).unsqueeze(0)[:, 0])\n",
        "        if buffer == None:\n",
        "          buffer = cos\n",
        "        else:\n",
        "          buffer = torch.cat([buffer, cos], axis=0)\n",
        "    return torch.mean(buffer)\n",
        "\n",
        "class CLIP_T_metric(BaseMetric):\n",
        "  def __init__(self, processor, text_model, vision_model):\n",
        "    super().__init__()\n",
        "    self.processor = processor\n",
        "    self.text_model = text_model\n",
        "    self.vision_model = vision_model\n",
        "\n",
        "  def compute(self, prompt, image):\n",
        "    with torch.no_grad():\n",
        "      vision_inputs = self.processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
        "      vision_emb = self.vision_model(vision_inputs)[1]\n",
        "      id, mask = self.processor(\n",
        "          text=prompt,\n",
        "          padding=\"max_length\",\n",
        "          return_tensors=\"pt\"\n",
        "      ).values()\n",
        "      text_emb = self.text_model(id.to(device), mask.to(device))[1]\n",
        "\n",
        "    return torch.abs(F.cosine_similarity(text_emb, vision_emb))\n",
        "\n",
        "class FID_metric():\n",
        "  def __init__(self, instance_path):\n",
        "    self.metric = FrechetInceptionDistance(feature=64, normalize=True).to(device)\n",
        "    self.preprocess = transforms.Compose([\n",
        "        transforms.Resize(512, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    for ins_path in list(Path(instance_path).iterdir()):\n",
        "        instance = Image.open(ins_path)\n",
        "        instance = exif_transpose(Image.open(ins_path))\n",
        "        instance = self.preprocess(instance).unsqueeze(0).to(device)\n",
        "        self.metric.update(instance, real=True)\n",
        "\n",
        "  def score(self):\n",
        "    return self.metric.compute()\n",
        "\n",
        "  def update(self, image, real=False):\n",
        "    image = self.preprocess(image).unsqueeze(0).to(device)\n",
        "    self.metric.update(image, real=real)\n",
        "\n",
        "  def reset(self):\n",
        "    self.metric.reset()\n",
        "\n",
        "  def print(self):\n",
        "    print(f\"{self.__class__.__name__} : {self.score()}\")\n",
        "\n",
        "class SSIM_metric(BaseMetric):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.metric = StructuralSimilarityIndexMeasure().to(device)\n",
        "    self.preprocess = transforms.Compose([\n",
        "        transforms.Resize(512, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "  def compute(self, instance_path, image):\n",
        "    buffer=None\n",
        "    image = self.preprocess(image).unsqueeze(0).to(device)\n",
        "    for ins_path in list(Path(instance_path).iterdir()):\n",
        "        instance = exif_transpose(Image.open(ins_path))\n",
        "        instance = self.preprocess(instance).unsqueeze(0).to(device)\n",
        "        score = torch.Tensor([self.metric(image, instance)])\n",
        "        if buffer == None:\n",
        "          buffer = score\n",
        "        else:\n",
        "          buffer = torch.cat([buffer, score], axis=0)\n",
        "    return torch.mean(buffer).to(device)\n",
        "\n",
        "class PSNR_metric(BaseMetric):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.metric = PeakSignalNoiseRatio().to(device)\n",
        "    self.preprocess = transforms.Compose([\n",
        "        transforms.Resize(512, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "  def compute(self, instance_path, image):\n",
        "    buffer=None\n",
        "    image = self.preprocess(image).unsqueeze(0).to(device)\n",
        "    for ins_path in list(Path(instance_path).iterdir()):\n",
        "        instance = exif_transpose(Image.open(ins_path))\n",
        "        instance = self.preprocess(instance).unsqueeze(0).to(device)\n",
        "        score = torch.Tensor([self.metric(image, instance)])\n",
        "        if buffer == None:\n",
        "          buffer = score\n",
        "        else:\n",
        "          buffer = torch.cat([buffer, score], axis=0)\n",
        "    return torch.mean(buffer).to(device)\n",
        "\n",
        "class LPIPS_metric(BaseMetric):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.metric = LearnedPerceptualImagePatchSimilarity(net_type='squeeze').to(device)\n",
        "    self.preprocess = transforms.Compose([\n",
        "        transforms.Resize(512, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "  def compute(self, instance_path, image):\n",
        "    buffer=None\n",
        "    image = self.preprocess(image).unsqueeze(0).to(device)\n",
        "    for ins_path in list(Path(instance_path).iterdir()):\n",
        "        instance = exif_transpose(Image.open(ins_path))\n",
        "        instance = self.preprocess(instance).unsqueeze(0).to(device)\n",
        "        score = torch.Tensor([self.metric(image, instance)])\n",
        "        if buffer == None:\n",
        "          buffer = score\n",
        "        else:\n",
        "          buffer = torch.cat([buffer, score], axis=0)\n",
        "    return torch.mean(buffer).to(device)\n",
        "\n",
        "def make_prompt(im_type, subject, background, style):\n",
        "  return f\"a {im_type} of {subject} {background} {style}\"\n",
        "\n",
        "im_type_list = [\"photo\", \"photo\", \"photo\", \"photo\", \"painting\"]\n",
        "background_list = [\"in sahara desert covered in sand\",\n",
        "              \"in a megacity with skyscrapers\",\n",
        "              \"in bali beach with temples on the back\",\n",
        "              \"in forest\",\n",
        "              \"in the middle of new york city times square\",\n",
        "              \"under the sea with corals\",\n",
        "              \"on top of mount everest covered in snow\",\n",
        "              \"in a field with cherry blossom trees\",\n",
        "              \"on the moon with galaxies background\",\n",
        "              \"infront of the eiffel tower\",\n",
        "              \"in a cyberpank alley with colorful neon lights\"]\n",
        "\n",
        "style_list = {\n",
        "    \"photo\":[\", realistic lighting, sharp, vibrant color\", \"\", \", realistic lighting\", \", dark and horror\", \", bright, cheerful, colorful\"],\n",
        "    \"painting\": [\", van gogh style\", \", colorful\", \", watercolor\", \", da vinci style\", \", renaissance painting\", \", expressionism\"]\n",
        "}"
      ],
      "metadata": {
        "id": "sAL0m5ybmWwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor_dino = ViTImageProcessor.from_pretrained('facebook/dino-vits16')\n",
        "model_dino = ViTModel.from_pretrained('facebook/dino-vits16').to(device)\n",
        "\n",
        "vision_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
      ],
      "metadata": {
        "id": "ybWbOOJEmgvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation"
      ],
      "metadata": {
        "id": "fS1w_URrm81y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Prompt Invariant\n",
        "dino = DINO_metric(processor_dino, model_dino)\n",
        "clip_i = CLIP_I_metric(processor, vision_model)\n",
        "clip_t = CLIP_T_metric(processor, pipe.text_encoder, vision_model)\n",
        "fid = FID_metric(INSTANCE_DIR)\n",
        "ssim = SSIM_metric()\n",
        "psnr = PSNR_metric()\n",
        "lpips = LPIPS_metric()\n",
        "\n",
        "pipe.set_progress_bar_config(disable=True)\n",
        "\n",
        "for i in tqdm(range(50)):\n",
        "  prompt = INSTANCE_PROMPT\n",
        "  image = pipe(prompt, num_inference_steps=50, guidance_scale=7).images[0]\n",
        "  dino.update(INSTANCE_DIR, image)\n",
        "  clip_i.update(INSTANCE_DIR, image)\n",
        "  clip_t.update(prompt, image)\n",
        "  fid.update(image)\n",
        "  ssim.update(INSTANCE_DIR, image)\n",
        "  psnr.update(INSTANCE_DIR, image)\n",
        "  lpips.update(INSTANCE_DIR, image)\n",
        "\n",
        "dino.print()\n",
        "clip_i.print()\n",
        "clip_t.print()\n",
        "fid.print()\n",
        "ssim.print()\n",
        "psnr.print()\n",
        "lpips.print()"
      ],
      "metadata": {
        "id": "dRikiKSmm-0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prompt Varying\n",
        "\n",
        "dino = DINO_metric(processor_dino, model_dino)\n",
        "clip_i = CLIP_I_metric(processor, vision_model)\n",
        "clip_t = CLIP_T_metric(processor, pipe.text_encoder, vision_model)\n",
        "fid = FID_metric(INSTANCE_DIR)\n",
        "ssim = SSIM_metric()\n",
        "psnr = PSNR_metric()\n",
        "lpips = LPIPS_metric()\n",
        "\n",
        "pipe.set_progress_bar_config(disable=True)\n",
        "\n",
        "for i in tqdm(range(50)):\n",
        "  im_type = random.choice(im_type_list)\n",
        "  background = random.choice(background_list)\n",
        "  style = random.choice(style_list[im_type])\n",
        "  prompt = make_prompt(im_type, f\"sks {CLASS_NAME}\", background, style)\n",
        "  image = pipe(prompt, num_inference_steps=50, guidance_scale=7).images[0]\n",
        "  dino.update(INSTANCE_DIR, image)\n",
        "  clip_i.update(INSTANCE_DIR, image)\n",
        "  clip_t.update(prompt, image)\n",
        "  fid.update(image)\n",
        "  ssim.update(INSTANCE_DIR, image)\n",
        "  psnr.update(INSTANCE_DIR, image)\n",
        "  lpips.update(INSTANCE_DIR, image)\n",
        "\n",
        "dino.print()\n",
        "clip_i.print()\n",
        "clip_t.print()\n",
        "fid.print()\n",
        "ssim.print()\n",
        "psnr.print()\n",
        "lpips.print()"
      ],
      "metadata": {
        "id": "r607EhgYndtK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}